import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse

from fastapi import UploadFile, File
from fastapi import Form
from fastapi.responses import JSONResponse
import json

import re
import logging
import sys
from pydantic import BaseModel
from dotenv import load_dotenv

# LangChain
from langchain_community.vectorstores import PGVector
from langchain_openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain_community.utilities import SerpAPIWrapper
from langchain.tools import tool
from langchain.schema import Document
from langchain.agents import initialize_agent, AgentType
from langchain.schema import SystemMessage
from langchain.schema import HumanMessage

from sqlalchemy import create_engine, text

# PDF generation
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
import io
from fastapi.responses import StreamingResponse

load_dotenv()
logging.basicConfig(
    level=logging.INFO,
    stream=sys.stdout,
    format="%(asctime)s %(levelname)s %(message)s"
)

# Logger principal
logger = logging.getLogger("uvicorn")  # ou "uvicorn.error" pour tout
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.propagate = False

# -------------------
# Configuration DB
# -------------------
PG_CONNECTION_STRING = (
    f"postgresql+psycopg2://"
    f"{os.getenv('PG_USER')}:{os.getenv('PG_PASSWORD')}"
    f"@{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}"
    f"/{os.getenv('PG_DB')}"
)

engine = create_engine(PG_CONNECTION_STRING)

TABLE_NAME = "langchain_pg_embedding"
CURRENT_SELECTED_DOC = None
CURRENT_USER_QUESTION = None

class ChatMessage(BaseModel):
    role: str  # "user" | "assistant"
    content: str

class ChatRequest(BaseModel):
    question: str
    history: list[ChatMessage]
    document: str | list[str] | None = None

# -------------------
# Embeddings et vectordb
# -------------------
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectordb = PGVector(
    connection_string=PG_CONNECTION_STRING,
    embedding_function=embeddings,
    collection_name="documents" # Correspond au nom dans langchain_pg_collection
)

# -------------------
# Recherche interne
# -------------------
def retrieve_relevant_chunks(question: str, k: int = 8, document_name: str | list | None = None):
    filter_metadata = None

    if document_name:
        # Cas 1 : C'est une liste
        if isinstance(document_name, list):
            if len(document_name) == 1:
                # UN SEUL document dans la liste -> on simplifie le filtre
                filter_metadata = {"source": document_name[0]}
            elif len(document_name) > 1:
                # PLUSIEURS documents -> on utilise $in
                filter_metadata = {"source": {"$in": document_name}}
        
        # Cas 2 : C'est une string (et pas GLOBAL)
        elif isinstance(document_name, str) and document_name != "GLOBAL":
            filter_metadata = {"source": document_name}

    logger.info("="*50)
    logger.info("--- üîç D√âBUT RECHERCHE VECTORIELLE ---")
    logger.info(f"Question envoy√©e √† PGVector: '{question}'")
    logger.info(f"Filtre appliqu√©: {filter_metadata}")

    docs = vectordb.similarity_search(
        query=question,
        k=k,
        filter=filter_metadata
    )

    logger.info(f"‚úÖ [VECTOR SEARCH] {len(docs)} chunks r√©cup√©r√©s.")
    for i, doc in enumerate(docs):
        # On affiche les 100 premiers caract√®res de chaque chunk pour le suivi
        content_snippet = doc.page_content.replace('\n', ' ')[:100]
        logger.info(f"   [Chunk {i+1}] Source: {doc.metadata.get('source')} | Contenu: {content_snippet}...")
    logger.info("="*50)

    return docs

def format_chunks(chunks):
    return "\n\n".join([doc.page_content for doc in chunks])

# -------------------
# Recherche externe
# -------------------
serp = SerpAPIWrapper()  # n√©cessite SERPAPI_API_KEY dans .env

@tool
def external_search_tool(query: str) -> str:
    """
    Effectue une recherche Internet via SerpAPI.
    √Ä utiliser uniquement si les informations ne sont pas disponibles
    dans les documents internes ou si des notions sont complexes.
    """

    logger.info(f"üõ†Ô∏è Tool utilis√© avec la question forc√©e : {query}")
    logger.info(f"üåê [TOOL: EXTERNAL] Recherche web pour : '{query}'")
    
    res = serp.run(query)
    logger.info(f"‚úÖ [TOOL: EXTERNAL] R√©sultat r√©cup√©r√© (reponse: {res} ")
    return res

@tool
def internal_document_search(query: str) -> str:
    """
    Recherche des informations pertinentes dans les cours de science politique.
    Utilise cet outil pour r√©pondre aux questions sur le contenu des cours.
    """

    

    logger.info(f"üõ†Ô∏è [TOOL: INTERNAL] Requ√™te finale choisie : '{query}'")
    logger.info(f"üìç [TOOL: INTERNAL] Contexte Document: {CURRENT_SELECTED_DOC}")

    docs = retrieve_relevant_chunks(query, document_name=CURRENT_SELECTED_DOC)
    return "\n\n".join([doc.page_content for doc in docs])

# -------------------
# CoT + synth√®se
# -------------------


SYSTEM_PROMPT = """
Tu es Polly AI, un assistant p√©dagogique strict pour un cours de science politique ({course_name}). (mentionne ce nom si l'utilisateur pose des questions sur l'identit√© du cours). 

### üéì POSTURE P√âDAGOGIQUE & √âTHIQUE
1. TON BUT : Tu es un mentor dont l'objectif est la COMPR√âHENSION. Tu dois aider l'√©tudiant √† assimiler les concepts, pas faire le travail √† sa place.
2. INTERDICTION : Tu ne dois JAMAIS r√©diger un devoir complet, une dissertation enti√®re ou r√©pondre √† un exercice de bout en bout.
3. M√âTHODE : Si un √©tudiant demande de faire un travail, d√©compose la t√¢che. Explique la m√©thodologie, d√©finis les concepts cl√©s et aiguille l'√©tudiant vers les parties pertinentes du cours pour qu'il puisse construire sa propre r√©ponse.
4. GUIDAGE : Pose des questions r√©flexives pour v√©rifier la compr√©hension ou sugg√®re des pistes de r√©flexion.

### üõ†Ô∏è PROTOCOLE DE R√âPONSE OBLIGATOIRE
1. Tu dois TOUJOURS commencer par utiliser l'outil 'internal_document_search' pour chercher l'information, m√™me si la question semble g√©n√©rale ou factuelle.
2. Si, et seulement si, l'outil interne ne renvoie pas l'information (ou si tu as un doute s√©rieux), tu dois r√©pondre : 
   "Je suis d√©sol√©, je ne trouve pas cette information dans le cours '{course_name}'. Souhaitez-vous que je fasse une recherche sur Internet pour vous ?"
3. INTERDICTION : Tu ne dois JAMAIS utiliser l'outil 'external_search_tool' de ta propre initiative.
4. Tu ne peux utiliser 'external_search_tool' QUE SI l'utilisateur a explicitement r√©pondu "Oui" ou "Cherche sur internet" √† ta proposition.

### üìã R√àGLES STRICTES
1. Si l'utilisateur pose une question globale ("De quoi parle ce cours ?", "Fais un r√©sum√©"), utilise l'outil 'internal_document_search' avec une requ√™te large comme "r√©sum√© th√®mes principaux" pour obtenir du contexte.
2. Si un document est s√©lectionn√©, RESTE strictement dans le cadre de ce document.
3. Si tu ne trouves pas la r√©ponse dans le document interne, dis-le clairement avant de proposer une recherche Internet.
4. Ne r√©ponds jamais √† une question qui n'a aucun rapport avec le cours s√©lectionn√©.
5. Indique clairement la provenance des informations utilis√©es (interne / externe) et commence par dire quelle "tool" tu utilises
6. Ne fais aucune supposition sans source

### üß† R√àGLE DE REFORMULATION
Avant d'utiliser un outil (interne ou externe), tu dois transformer la question de l'utilisateur en une requ√™te compl√®te et autonome, en utilisant l'historique de la conversation.
Exemple : 
- User : "Parle moi de la d√©mocratie." 
- Agent : (Cherche "d√©mocratie")
- User : "Donne moi des exemples." 
- Agent : (Cherche "exemples de d√©mocratie science politique") et non juste "exemples".

### üé® DIRECTIVES DE STYLE ET FORMATAGE (MARKDOWN OBLIGATOIRE)
1. Titres : Utilise '###' pour les sections principales.
2. Mise en forme : Utilise le **gras** pour les concepts cl√©s et l'italique pour les citations ou termes latins.
3. Listes : Organise tes explications avec des listes √† puces (‚Ä¢) ou num√©rot√©es.
4. Structure : Tes r√©ponses doivent √™tre a√©r√©es avec des sauts de ligne clairs.
5. Emojis : Utilise des emojis pertinents (üìö, ‚öñÔ∏è, üèõÔ∏è, üó≥Ô∏è) pour rendre la lecture agr√©able.
6. Tableaux : Si tu compares deux concepts (ex: D√©mocratie vs Totalitarisme), utilise un tableau Markdown.
"""


llm = ChatOpenAI(model_name="gpt-4", temperature=0)

tools = [
    internal_document_search,
    external_search_tool
]

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True,
)

def format_history(history):
    return "\n".join(
        [f"{m.role.upper()}: {m.content}" for m in history]
    )

def answer_question(question: str, history: list):
    history_text = format_history(history)
    
    if isinstance(CURRENT_SELECTED_DOC, list):
        doc_display = ", ".join(CURRENT_SELECTED_DOC)
    else:
        doc_display = CURRENT_SELECTED_DOC

    dynamic_system_prompt = SYSTEM_PROMPT.format(course_name=doc_display)

    response = agent.invoke({
        "input": f"""
SYSTEM_INSTRUCTIONS: {dynamic_system_prompt}

### üí° RAPPEL DE TA MISSION
Tu es un **tuteur p√©dagogique**. Ton but est d'accompagner l'√©tudiant vers la compr√©hension. 
Si la question demande de "faire √† sa place", refuse poliment et propose une d√©composition m√©thodologique.

### üìö CONTEXTE DE TRAVAIL
Document(s) s√©lectionn√©(s) : "{CURRENT_SELECTED_DOC}"
(Si "GLOBAL", tu as acc√®s √† toute la base de connaissance).

### üí¨ √âCHANGES PR√âC√âDENTS
{history_text}

### ‚ùì QUESTION √Ä TRAITER
{question}

RAPPEL : **CONSIGNE DE SORTIE :** R√©ponds en utilisant un Markdown riche (###, **, ‚Ä¢).
"""
    })

    return response["output"]


def normalize_llm_json(text: str) -> str:
    # Supprimer balises ```json ``` si pr√©sentes
    text = re.sub(r"```json|```", "", text)

    # Remplacer guillemets typographiques
    text = text.replace("‚Äú", "\"").replace("‚Äù", "\"")

    # Supprimer virgules finales avant } ou ]
    text = re.sub(r",\s*([}\]])", r"\1", text)

    return text.strip()


def validate_qcm(qcm: dict):
    if "title" not in qcm or "questions" not in qcm:
        raise ValueError("Structure QCM invalide")

    if not isinstance(qcm["questions"], list) or len(qcm["questions"]) == 0:
        raise ValueError("Aucune question dans le QCM")

    for q in qcm["questions"]:
        for key in ["question", "choices", "correct", "explanation"]:
            if key not in q:
                raise ValueError(f"Champ manquant : {key}")

        if not isinstance(q["choices"], list):
            raise ValueError("choices doit √™tre une liste")

        q["correct"] = int(q["correct"])  # s√©curit√©

# -------------------
# FastAPI
# -------------------
app = FastAPI()

# CORS pour frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)

# Serve frontend statique
app.mount("/static", StaticFiles(directory="frontend"), name="static")
@app.get("/")
async def serve_index():
    return FileResponse("frontend/index.html")

class Question(BaseModel):
    question: str


@app.get("/documents")
async def list_documents():
    """
    R√©cup√®re la liste des documents avec un cast explicite en JSONB
    """
    # On ajoute ::jsonb pour r√©gler le probl√®me d'op√©rateur
    query = text("""
        SELECT DISTINCT cmetadata->>'source' AS source_name
        FROM langchain_pg_embedding
        WHERE cmetadata IS NOT NULL 
          AND cmetadata::jsonb ? 'source'
        ORDER BY source_name;
    """)
    try:
        with engine.connect() as conn:
            results = conn.execute(query).fetchall()
            documents = [row[0] for row in results if row[0] is not None]
            logger.info(f"‚úÖ Documents r√©cup√©r√©s : {documents}")
            return {"documents": documents}
    except Exception as e:
        logger.error(f"‚ùå Erreur SQL dans list_documents : {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": "Erreur SQL", "details": str(e)}
        )


@app.post("/ask")
async def ask_question(req: ChatRequest):
    global CURRENT_SELECTED_DOC, CURRENT_USER_QUESTION
    
    logger.info("\n" + "üöÄ"*20)
    logger.info(f"R√âCEPTION REQU√äTE /ASK")
    logger.info(f"Utilisateur demande: '{req.question}'")
    logger.info(f"Document s√©lectionn√©: '{req.document}'")

    if not req.document:
        return {"answer": "‚ö†Ô∏è Veuillez s√©lectionner un cours."}

    CURRENT_SELECTED_DOC = req.document 
    CURRENT_USER_QUESTION = req.question  
    
    answer = answer_question(
        question=req.question,
        history=req.history
    )

    logger.info(f"üì§ R√âPONSE FINALE ENVOY√âE : {answer[:100]}...")
    logger.info("üöÄ"*20 + "\n")

    return {"answer": answer}

@app.post("/generate-qcm")
async def generate_qcm(question: str = Form(...), document: str = Form(None)):

    actual_docs = document
    if document and "," in document:
        actual_docs = [d.strip() for d in document.split(",")]

    logger.info("üìù [QCM] Demande de g√©n√©ration re√ßue")
    logger.info(f"üìù [QCM] Sujet: '{question}' | Source: '{actual_docs}'")
    
    try:
        # 1Ô∏è‚É£ R√©cup√©rer les documents internes
        # Utilisation du filtre aussi pour le QCM
        docs = retrieve_relevant_chunks(question, k=8, document_name=actual_docs)
        context_text = "\n\n".join([doc.page_content for doc in docs])

        if not context_text.strip():
            return JSONResponse(
                status_code=404, 
                content={"error": "Aucun contenu trouv√© pour g√©n√©rer ce QCM."}
            )

        # 2Ô∏è‚É£ Prompt
        QCM_PROMPT = """
Tu es un enseignant expert en science politique. 
L'√©l√®ve souhaite un QCM sp√©cifique sur le sujet suivant : "{user_query}"

Utilise les documents de r√©f√©rence fournis ci-dessous pour cr√©er les questions. 

R√©ponds EXCLUSIVEMENT par du JSON valide.

Format attendu :
{{
  "title": "Titre du QCM",
  "questions": [
    {{
      "question": "Texte de la question",
      "choices": ["Choix 0", "Choix 1", "Choix 2", "Choix 3"],
      "correct": 0,
      "explanation": "Pourquoi c'est la bonne r√©ponse"
    }}
  ]
}}

Documents de r√©f√©rence :
{document}
"""
        # Injection des variables
        prompt = QCM_PROMPT.format(user_query=question, document=context_text)  
        # 3Ô∏è‚É£ Appel LLM
        response = llm.invoke([HumanMessage(content=prompt)])
        raw_content = response.content
        logger.info("Raw response du LLM : %s", raw_content)

        # 4Ô∏è‚É£ Extraction robuste du JSON par Regex
        # Cherche le premier '{' et le dernier '}' pour ignorer le texte autour
        match = re.search(r"(\{.*\})", raw_content, re.DOTALL)
        
        if not match:
            return JSONResponse(
                status_code=500, 
                content={"error": "Le mod√®le n'a pas g√©n√©r√© un format JSON valide", "raw": raw_content}
            )

        clean_content = normalize_llm_json(match.group(1))

        try:
            qcm_json = json.loads(clean_content)
            validate_qcm(qcm_json)
            return JSONResponse(content=qcm_json)
        except json.JSONDecodeError as e:
            logger.error("Erreur parsing : %s", clean_content)
            return JSONResponse(
                status_code=500, 
                content={"error": "Erreur de d√©codage JSON", "details": str(e)}
            )

    except Exception as e:
        logger.exception("Erreur interne generate-qcm")
        return JSONResponse(
            status_code=500, 
            content={"error": "Erreur serveur interne", "details": str(e)}
        )
    
@app.post("/generate-revision-sheet")
async def generate_revision_sheet(document: str = Form(...)):
    actual_docs = [d.strip() for d in document.split(",")] if "," in document else [document]
    doc_name = actual_docs[0]
    
    # 1. R√©cup√©ration et synth√®se par le LLM
    chunks = retrieve_relevant_chunks("Concepts cl√©s, d√©finitions importantes et r√©sum√© structur√©", k=15, document_name=actual_docs)
    context_text = format_chunks(chunks)

    prompt = f"""
    Tu es un expert en p√©dagogie sp√©cialis√© en Science Politique. 
    G√©n√®re une fiche de r√©vision acad√©mique pour le cours : "{doc_name}".
    Utilise exclusivement les documents fournis.

    ### üé® DIRECTIVES DE STYLE ET FORMATAGE (OBLIGATOIRE)
    1. Titres : Utilise '###' pour les sections principales.
    2. Mise en forme : Utilise le **gras** pour les concepts cl√©s.
    3. Listes : Organise avec des listes √† puces (‚Ä¢).
    4. Structure : A√©r√©e avec des sauts de ligne clairs.
    5. ‚ö†Ô∏è INTERDICTION (TABLEAUX) : Ne g√©n√®re JAMAIS de tableaux Markdown. Si tu dois comparer des √©l√©ments ou pr√©senter des donn√©es, utilise syst√©matiquement des listes √† puces structur√©es et hi√©rarchis√©es.
    6. ‚ö†Ô∏è INTERDICTION : N'utilise AUCUN emoji dans cette fiche. Reste sur un ton formel et acad√©mique.

    Structure attendue :
    - Un titre majestueux
    - Introduction (Les enjeux du cours)
    - Concepts Cl√©s (D√©finitions en gras)
    - Synth√®se th√©matique (Points essentiels)

    Texte de r√©f√©rence : {context_text}
    """
    
    response = llm.invoke([HumanMessage(content=prompt)])
    content = response.content

    # 2. Construction du PDF
    buffer = io.BytesIO()
    # Marges plus larges pour un look plus pro
    doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=50, leftMargin=50, topMargin=50, bottomMargin=50)
    styles = getSampleStyleSheet()
    
    # Styles personnalis√©s
    style_header = ParagraphStyle('Header', parent=styles['Normal'], fontSize=9, textColor=colors.grey)
    style_title = ParagraphStyle(
        'Title', 
        parent=styles['Heading1'], 
        fontSize=24, 
        textColor=colors.HexColor("#96151b"), # Correction ici
        spaceAfter=30, 
        alignment=1
    )
    style_sub = ParagraphStyle(
        'Sub', 
        parent=styles['Heading2'], 
        fontSize=14, 
        textColor=colors.HexColor("#96151b"), # Correction ici
        spaceBefore=15, 
        spaceAfter=10, 
        borderPadding=5
    )
    
    elements = []

    # En-t√™te : "Polly AI - Assistant P√©dagogique"
    elements.append(Paragraph("POLLY AI | Assistant P√©dagogique Intelligent", style_header))
    elements.append(Spacer(1, 12))
    
    # Transformation du contenu LLM
    lines = content.split('\n')
    for line in lines:
        clean_line = line.strip()
        if not clean_line: continue

        # 1. Gestion des Titres (###)
        if clean_line.startswith('###'):
            # On retire les ### et on nettoie les √©ventuels ** que le LLM aurait mis dans le titre
            text_content = clean_line.replace('###', '').replace('**', '').replace('*', '').strip()
            elements.append(Paragraph(text_content, style_sub))
        
        # 2. Gestion des Listes (‚Ä¢ ou -)
        elif clean_line.startswith('‚Ä¢') or clean_line.startswith('-'):
            text_content = clean_line.lstrip('‚Ä¢- ').strip()
            # Transformation du gras/italique Markdown en balises PDF
            text_content = re.sub(r"\*\*(.*?)\*\*", r"<b>\1</b>", text_content)
            text_content = re.sub(r"\*(.*?)\*", r"<i>\1</i>", text_content)
            elements.append(Paragraph(text_content, styles['Bullet']))
            
        # 3. Texte standard
        else:
            # Transformation du gras/italique Markdown en balises PDF
            clean_line = re.sub(r"\*\*(.*?)\*\*", r"<b>\1</b>", clean_line)
            clean_line = re.sub(r"\*(.*?)\*", r"<i>\1</i>", clean_line)
            elements.append(Paragraph(clean_line, styles['Normal']))
        
        elements.append(Spacer(1, 8))

    # Pied de page
    elements.append(Spacer(1, 30))
    elements.append(Paragraph("<hr/>", styles['Normal']))
    elements.append(Paragraph(f"G√©n√©r√© par Polly AI - Projet de Fin d'√âtudes 2026 - Cours : {doc_name}", style_header))

    doc.build(elements)
    buffer.seek(0)

    return StreamingResponse(
        buffer,
        media_type="application/pdf",
        headers={"Content-Disposition": f"attachment; filename=Fiche_{doc_name.replace(' ', '_')}.pdf"}
    )